{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83e\udde0 ResNet23 CIFAR-10 Classifier","text":"<p>This project implements an image classification pipeline using a custom ResNet23 model with 23 layers starting from the original resnet18 on a downsampled CIFAR-10 dataset.</p>"},{"location":"#project-structure","title":"\ud83d\udcc1 Project Structure","text":"<pre><code>\u251c\u2500\u2500 LICENSE            &lt;- Open-source license if one is chosen\n\u251c\u2500\u2500 Makefile           &lt;- Makefile with convenience commands like `make data` or `make train`\n\u251c\u2500\u2500 README.md          &lt;- The top-level README for developers using this project.\n\u251c\u2500\u2500 data\n\u2502   \u251c\u2500\u2500 external       &lt;- Data from third party sources.\n\u2502   \u251c\u2500\u2500 interim        &lt;- Intermediate data that has been transformed.\n\u2502   \u251c\u2500\u2500 processed      &lt;- The final, canonical data sets for modeling.\n\u2502   \u2514\u2500\u2500 raw            &lt;- The original, immutable data dump.\n\u2502\n\u251c\u2500\u2500 docs               &lt;- A default mkdocs project; see www.mkdocs.org for details\n\u2502\n\u251c\u2500\u2500 models             &lt;- Trained and serialized models, model predictions, or model summaries\n\u2502\n\u251c\u2500\u2500 notebooks          &lt;- Jupyter notebooks. Naming convention is a number (for ordering),\n\u2502                         the creator's initials, and a short `-` delimited description, e.g.\n\u2502                         `1.0-jqp-initial-data-exploration`.\n\u2502\n\u251c\u2500\u2500 pyproject.toml     &lt;- Project configuration file with package metadata for \n\u2502                         Resnet23 and configuration for tools like black\n\u2502\n\u251c\u2500\u2500 references         &lt;- Data dictionaries, manuals, and all other explanatory materials.\n\u2502\n\u251c\u2500\u2500 reports            &lt;- Generated analysis as HTML, PDF, LaTeX, etc.\n\u2502   \u2514\u2500\u2500 figures        &lt;- Generated graphics and figures to be used in reporting\n\u2502\n\u251c\u2500\u2500 requirements.txt   &lt;- The requirements file for reproducing the analysis environment, e.g.\n\u2502                         generated with `pip freeze &gt; requirements.txt`\n\u2502\n\u251c\u2500\u2500 setup.cfg          &lt;- Configuration file for flake8\n\u2502\n\u2514\u2500\u2500 Resnet23   &lt;- Source code for use in this project.\n    \u2502\n    \u251c\u2500\u2500 __init__.py             &lt;- Makes Resnet23 a Python module\n    \u2502\n    \u251c\u2500\u2500 config.py               &lt;- Store useful variables and configuration and dagshub access \n    \u2502\n    \u251c\u2500\u2500 datadownload.py              &lt;- Scripts to download data\n    \u2502\n    \u251c\u2500\u2500 features.py             &lt;- Code to create features for modeling\n    \u2502\n    \u251c\u2500\u2500 modeling                \n    \u2502   \u251c\u2500\u2500 __init__.py \n    \u2502   \u251c\u2500\u2500 predict.py          &lt;- Code to run model inference with trained models          \n    \u2502   \u2514\u2500\u2500 train.py            &lt;- Code to train models\n    \u2502\n    \u2514\u2500\u2500 plots.py                &lt;- Code to create visualizations\n</code></pre>"},{"location":"#setup","title":"Setup","text":""},{"location":"#create-conda-environment","title":"Create Conda Environment","text":"<pre><code>conda create -n Resnet23 python=3.10\nconda activate Resnet23\n</code></pre>"},{"location":"#install-requirements","title":"Install Requirements","text":"<pre><code>pip install torch torchvision tqdm\n</code></pre>"},{"location":"#configuration","title":"Configuration","text":"<p>Defined in <code>Resnet23/config.py</code>:</p> <pre><code>DATA_DIR = \"data/processed\"\nIMAGE_SIZE = (32, 32)\nBATCH_SIZE = 64\nNUM_CLASSES = 10\nCLASS_NAMES = [\"airplane\", \"automobile\", ..., \"truck\"]\n</code></pre> <p>This helps keep training code clean and centralized.</p>"},{"location":"#download-downsample-cifar-10","title":"Download &amp; Downsample CIFAR-10","text":"<p>Run this script to download CIFAR-10 and save a limited number of images per class into a folder-based structure: - Downloads the dataset into data/raw. - Creates target directories if they don't exist. - Iterates over (image, label) pairs. - Converts image from tensor to PIL using ToPILImage. - Resizes image to IMAGE_SIZE (usually 32\u00d732). - Saves image to output_dir/split/class_name/filename.png.</p>"},{"location":"#arguments","title":"Arguments :","text":"<ul> <li>output_dir (str): Destination directory for processed data .</li> <li>max_per_class (int): Maximum number of images to save for each class (default is 500).</li> </ul> <pre><code>python Resnet23/download_data.py\n</code></pre> <p>This creates:</p> <pre><code>data/processed/\n  \u251c\u2500\u2500 train/\n  \u2502   \u251c\u2500\u2500 airplane/\n  \u2502   \u251c\u2500\u2500 ...\n  \u2514\u2500\u2500 test/\n      \u251c\u2500\u2500 airplane/\n      \u251c\u2500\u2500 ...\n</code></pre>"},{"location":"#4-dataset-loader","title":"\ud83e\uddfe 4. Dataset Loader","text":"<p>Implemented in <code>Resnet23/dataset.py</code>:</p> <ul> <li><code>CIFARCustomDataset</code>: Loads images from folder structure.</li> <li><code>get_default_transform()</code>: Defines transforms (resize, normalize).</li> </ul>"},{"location":"#parameters","title":"Parameters:","text":"<ul> <li>root_dir (str): Path to the train/ or test/ directory containing class-named folders.</li> <li>transform: A set of transforms to apply to each image (e.g., resizing, normalization).</li> </ul>"},{"location":"#process","title":"Process:","text":"<ul> <li>Iterates over all class folders.</li> <li>Maps each .png image to its label index.</li> <li>Stores:</li> <li>self.image_paths: List of full paths to all images.</li> <li> <p>self.labels: Corresponding label indices.</p> </li> <li> <p>getitem : </p> <ul> <li>Loads image path from self.image_paths.</li> <li>Reads the image using PIL.Image.open().</li> <li>Converts it to RGB.</li> <li>Applies any provided transform.</li> <li>Returns the image tensor and its label.</li> </ul> </li> </ul>"},{"location":"#model-architecture","title":"Model Architecture","text":"<p>Defined in <code>Resnet23/modeling/model23.py</code>:</p> <p>This file contains a custom implementation of a 23-layer ResNet (ResNet-23) based on the original ResNet architecture. The model extends the standard ResNet-18 design with additional layers and modern training enhancements to improve performance on our task.</p>"},{"location":"#model-specifications","title":"Model Specifications","text":"<ul> <li>Total Layers: 23 convolutional layers</li> <li>Block Type: BasicBlock (2 conv layers per block)</li> <li>Layer Configuration: [3, 3, 3, 2] blocks per layer</li> <li>Parameters: ~11.7M (compared to ~11.2M in ResNet-18)</li> <li>input Size:  32\u00d732 for CIFAR-10</li> </ul>"},{"location":"#architectural-details","title":"Architectural Details :","text":""},{"location":"#1-layer-count","title":"1. Layer Count","text":"<ul> <li>ResNet-18: [2, 2, 2, 2] blocks = 18 layers</li> <li>ResNet-23: [3, 3, 3, 2] blocks = 23 layers</li> <li>Impact: Increased model  and  power</li> </ul>"},{"location":"#2-input-for-small-images","title":"2. Input for Small Images","text":"<ul> <li>Traditional: 7\u00d77 conv, stride=2, MaxPool \u2192 Reduces 224\u00d7224 to 56\u00d756</li> <li>ResNet-23: 3\u00d73 conv, stride=1, no MaxPool \u2192 Preserves 32\u00d732 resolution</li> <li>Benefit : Prevents  downsampling of small images</li> </ul>"},{"location":"#3-dropout","title":"3. Dropout","text":"<ul> <li>Block Dropout: 2D dropout (default 10%) within residual blocks</li> <li>Final Dropout: Additional 20% dropout before classification</li> <li>Benifit: Prevents overfitting on small datasets</li> </ul>"},{"location":"#4-make-layer-method","title":"4. Make Layer method","text":"<ul> <li>This method creates a sequence of residual blocks (BasicBlock) that form one layer of the ResNet.</li> <li>Checks if downsampling is needed (if stride != 1 or channels change).If yes, creates a shortcut (downsample).</li> <li>Adds the first block (handles the dimension change).</li> <li>Adds more blocks (keeps dimensions the same).</li> <li>Returns the full stack as a nn.Sequential layer. <pre><code>Input (64 channels, 32x32)\n\u2502\n\u25bc\n[Block 1] \u2192 Changes to 128 channels, size 16x16 (stride=2)\n\u2502\n\u25bc\n[Block 2] \u2192 Keeps 128 channels, 16x16\n\u2502\n\u25bc\n[Block 3] \u2192 Keeps 128 channels, 16x16\n\u2502\n\u25bc\nOutput (128 channels, 16x16)\n</code></pre></li> </ul>"},{"location":"#5-advanced-weight","title":"5. Advanced Weight","text":"<p>Initializes the neural network weights for better training performance. <pre><code>def _initialize_weights(self):\n    for m in self.modules():\n        if isinstance(m, nn.Conv2d):\n            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n        elif isinstance(m, nn.BatchNorm2d):\n            nn.init.constant_(m.weight, 1)\n            nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.Linear):\n            nn.init.normal_(m.weight, 0, 0.01)\n            nn.init.constant_(m.bias, 0)\n</code></pre> -  Kaiming Normal Initialization (best for ReLU in Conv layers) -  BatchNorm Initialization (sets weights=1, biases=0) -  Linear Layer Initialization (small random weights for stability)</p>"},{"location":"#6-training-script","title":"\ud83c\udfcb\ufe0f\u200d\u2642\ufe0f 6. Training Script","text":"<p>Located in <code>Resnet23/train.py</code>:</p> <ul> <li>Loads data using <code>DataLoader</code></li> <li>Trains model using <code>CrossEntropyLoss</code> and <code>Adam</code></li> <li>Tracks train/test accuracy</li> <li>Saves model to <code>models/resnet23_cifar.pth</code></li> </ul> <p>Run training:</p> <pre><code>python Resnet23/train.py\n</code></pre>"},{"location":"#results","title":"Results","text":"<p>Training the model for 20 epochs with total of 5000 images gave us :   - 50% Train accuracy / 40% Test accuracy --&gt; Using Typical Resnet18.   - 91% Train accuracy / 58% Test Accuracy --&gt; Using modified Resnet23.</p>"},{"location":"#discussion","title":"Discussion","text":"<p>The main reason why the model gave us a high training accuracy and lower test accuracy is that the model was too powerful for our small dataset so it memorized our data which lead us to have a significant difference betweeen train and test . - Solution : train it on more data will resolve the issue .</p>"},{"location":"#update-20","title":"Update 2.0","text":""},{"location":"#1-added-validation-split-and-evaluation","title":"1. Added Validation Split and Evaluation","text":"<ul> <li>The original dataset is now split into training and validation sets using <code>torch.utils.data.random_split</code>.</li> <li>A new <code>validate(epoch)</code> method was introduced to compute validation metrics per epoch.</li> <li> <p>Train/val dataset now apply different transforms :</p> <ul> <li>train set uses <code>get_train_transform</code> val uses <code>get_default_transform</code></li> </ul> </li> <li> <p>Why We Did This ?</p> <ul> <li>To monitor generalization and prevent overfitting during training.</li> </ul> </li> </ul>"},{"location":"#2-added-matrics-to-compute-evaluation-metrics","title":"2. Added matrics to Compute Evaluation Metrics","text":"<p>A new <code>metrics.py</code> file was added to compute and log: - Precision, Recall, F1-score per class - Mean Average Precision (mAP) using average_precision_score - Confusion Matrix as a visual artifact (confusion_matrix.png) - All metrics are automatically logged to DagsHub via MLflow Why We Did This ? -  Accuracy alone is not enough; we need per-class evaluation. - mAP is useful to evaluate how well the model ranks predictions. - Confusion matrix helps visualize class-wise performance.</p>"},{"location":"#3-full-experiment-tracking-with-dagshub-and-mlflow","title":"3. Full Experiment Tracking with DagsHub and MLflow","text":"<ul> <li> <p>mlflow.start_run() is used to log:</p> <ul> <li>Model parameters (batch_size, optimizer, learning_rate, epochs)</li> <li>Per-epoch metrics (train/val loss &amp; accuracy)</li> <li>Final model file and confusion matrix</li> <li>Precision/recall/F1/mAP after training</li> </ul> </li> <li> <p>Configuration:</p> <ul> <li>DagsHub MLflow tracking URI is set using: <code>mlflow.set_tracking_uri(\"https://dagshub.com/&lt;username&gt;/&lt;repo&gt;.mlflow\")</code></li> </ul> </li> <li>Model artifacts saved to: <code>torch.save(model.state_dict(), \"models/resnet23_cifar.pth\") mlflow.log_artifact(\"models/resnet23_cifar.pth\")</code></li> <li>Why We Did This ?<ul> <li>Enables reproducibility and remote experiment tracking.</li> <li>Keeps track of hyperparameters, model artifacts, and validation/test performance.</li> </ul> </li> </ul>"},{"location":"#4-added-image-augmentation-for-better-generalization","title":"4. Added Image Augmentation for Better Generalization","text":"<ul> <li>A new function get_train_transform() was added to apply YOLO-style data augmentation:<ul> <li>RandomHorizontalFlip</li> <li>ColorJitter (brightness, contrast, saturation, hue)</li> <li>RandomAffine (rotation and translation)</li> </ul> </li> <li>Applied only to the training set, not to validation/test sets</li> <li>Why we did this ?<ul> <li>Boosts model generalization on unseen data.</li> <li>Makes the network robust to variations like rotation, brightness, and alignment.</li> <li>prevent overfitting </li> </ul> </li> </ul>"},{"location":"#results-discussion","title":"Results Discussion :","text":"<ul> <li>everything now could be tracked throught the repo in the dagsuhb page that could be found in this link : Resnet23_Expirement</li> <li>the confusion matrix graph could be found in : Confusion Matrix</li> </ul>"}]}